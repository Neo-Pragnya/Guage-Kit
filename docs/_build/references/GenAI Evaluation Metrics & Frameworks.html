

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>🧑‍🏫 Evaluation Metrics &amp; Frameworks — A Practical Guide &mdash; Guage-Kit  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Guage-Kit
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guides/installing.html">Installing Guage-Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides/llm_metrics.html">LLM Metrics in Guage-Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides/rag_metrics.html">RAG Metrics Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides/embeddings_metrics.html">Embeddings Metrics Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides/choosing_metrics.html">Choosing Metrics for Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides/reproducibility.html">Reproducibility in Evaluations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">Guage-Kit API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Guage-Kit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">🧑‍🏫 Evaluation Metrics &amp; Frameworks — A Practical Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/references/GenAI Evaluation Metrics &amp; Frameworks.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="evaluation-metrics-frameworks-a-practical-guide">
<h1>🧑‍🏫 Evaluation Metrics &amp; Frameworks — A Practical Guide<a class="headerlink" href="#evaluation-metrics-frameworks-a-practical-guide" title="Link to this heading"></a></h1>
<p>When we build NLP or GenAI systems, one of the hardest challenges is <strong>measuring quality</strong>. Unlike simple ML models where accuracy might suffice, in NLG/NLU we need a mix of <strong>lexical, semantic, factual, diversity, and operational metrics</strong>.</p>
<p>Think of it like teaching a student:</p>
<ul class="simple">
<li><p>First, you check whether they repeat the <em>exact words</em> (lexical).</p></li>
<li><p>Next, you check if they capture the <em>meaning</em> (semantic).</p></li>
<li><p>Then you test if they can separate <strong>facts from imagination</strong> (factuality/hallucinations).</p></li>
<li><p>After that, you grade them on <strong>creativity, safety, and fairness</strong> (diversity &amp; bias).</p></li>
<li><p>Finally, you grade them not just on answers, but also <strong>speed, memory, and usability</strong> (operational metrics).</p></li>
</ul>
<p>Let’s go step by step.</p>
<hr class="docutils" />
<section id="classical-nlg-metrics-lexical-overlap">
<h2>🔹 1. Classical NLG Metrics (Lexical Overlap)<a class="headerlink" href="#classical-nlg-metrics-lexical-overlap" title="Link to this heading"></a></h2>
<p>These are the “first-generation” metrics — they check <strong>word overlap</strong> with a reference text.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Weaknesses</p></th>
<th class="head"><p>When to use</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>ROUGE</strong></p></td>
<td><p>Recall of overlapping n-grams and sequences</p></td>
<td><p>ROUGE-N (n-grams), ROUGE-L (longest subsequence), ROUGE-S (skip-bigrams)</p></td>
<td><p>Simple, widely used; recall-sensitive</p></td>
<td><p>Ignores synonyms/paraphrases</p></td>
<td><p>Summarization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>BLEU</strong></p></td>
<td><p>Precision of overlapping n-grams</p></td>
<td><p>Modified precision + brevity penalty</p></td>
<td><p>MT standard; easy to compare</p></td>
<td><p>Biased to short, surface matches</p></td>
<td><p>Machine translation</p></td>
</tr>
<tr class="row-even"><td><p><strong>METEOR</strong></p></td>
<td><p>Word overlap + synonyms/stems</p></td>
<td><p>Alignment using WordNet</p></td>
<td><p>Higher correlation w/ humans</p></td>
<td><p>Still surface-biased</p></td>
<td><p>MT, summarization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>CIDEr</strong></p></td>
<td><p>Consensus similarity across multiple references</p></td>
<td><p>TF-IDF weighted n-grams</p></td>
<td><p>Great for captioning</p></td>
<td><p>Needs multiple refs</p></td>
<td><p>Image captioning</p></td>
</tr>
</tbody>
</table>
<p>👉 <strong>Industrial use</strong>: Good for <em>benchmark papers</em> and quick baselines, but insufficient for modern generative models (they can paraphrase well and still get low ROUGE/BLEU).</p>
</section>
<hr class="docutils" />
<section id="semantic-neural-metrics">
<h2>🔹 2. Semantic / Neural Metrics<a class="headerlink" href="#semantic-neural-metrics" title="Link to this heading"></a></h2>
<p>Second-generation metrics capture <strong>meaning, not just words</strong>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Weaknesses</p></th>
<th class="head"><p>When to use</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>BERTScore</strong></p></td>
<td><p>Semantic similarity</p></td>
<td><p>Cosine similarity of contextual embeddings</p></td>
<td><p>Captures paraphrase &amp; semantics</p></td>
<td><p>Needs heavy encoders</p></td>
<td><p>QA, abstractive summarization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>BLEURT</strong></p></td>
<td><p>Learned human-like scores</p></td>
<td><p>Fine-tuned BERT on human ratings</p></td>
<td><p>High correlation w/ human eval</p></td>
<td><p>Black box</p></td>
<td><p>General NLG</p></td>
</tr>
<tr class="row-even"><td><p><strong>MoverScore</strong></p></td>
<td><p>Soft alignment of words</p></td>
<td><p>Word Mover’s Distance on embeddings</p></td>
<td><p>Handles synonyms well</p></td>
<td><p>Slower than ROUGE</p></td>
<td><p>Summarization, QA</p></td>
</tr>
<tr class="row-odd"><td><p><strong>COMET</strong></p></td>
<td><p>Quality for MT/QG</p></td>
<td><p>Regression from pretrained encoders to human scores</p></td>
<td><p>SOTA for MT</p></td>
<td><p>Task-specific, heavy</p></td>
<td><p>Translation tasks</p></td>
</tr>
<tr class="row-even"><td><p><strong>QuestEval</strong></p></td>
<td><p>Factual consistency via QA</p></td>
<td><p>Generate QA pairs and compare answers</p></td>
<td><p>Targets factuality directly</p></td>
<td><p>Dependent on QA model</p></td>
<td><p>Summarization, fact checking</p></td>
</tr>
</tbody>
</table>
<p>👉 <strong>Industrial use</strong>: Preferred for <strong>customer-facing apps</strong> where <em>meaning</em> matters more than exact words. For example, evaluating a chatbot that paraphrases.</p>
</section>
<hr class="docutils" />
<section id="hallucination-factuality-metrics">
<h2>🔹 3. Hallucination &amp; Factuality Metrics<a class="headerlink" href="#hallucination-factuality-metrics" title="Link to this heading"></a></h2>
<p>Large models often <strong>“make things up”</strong> (hallucinations). Here’s how we measure and control that:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Weaknesses</p></th>
<th class="head"><p>When to use</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Faithfulness</strong></p></td>
<td><p>Alignment with retrieved context</p></td>
<td><p>Check each sentence against retrieval docs</p></td>
<td><p>Direct grounding</p></td>
<td><p>Needs context</p></td>
<td><p>RAG / GraphRAG</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Factual Consistency</strong></p></td>
<td><p>Logical entailment vs source</p></td>
<td><p>NLI or QA-based</p></td>
<td><p>Captures hallucinations</p></td>
<td><p>Sensitive to model errors</p></td>
<td><p>Summarization</p></td>
</tr>
<tr class="row-even"><td><p><strong>Answer Relevancy</strong></p></td>
<td><p>Does answer address the question?</p></td>
<td><p>LLM judge or embeddings</p></td>
<td><p>Easy to apply</p></td>
<td><p>Doesn’t check factuality</p></td>
<td><p>QA agents</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Unsupported Claim Rate</strong></p></td>
<td><p>% of unsupported statements</p></td>
<td><p>NLI or LLM judge</p></td>
<td><p>Strong guardrail</p></td>
<td><p>Needs strong judge</p></td>
<td><p>Safety pipelines</p></td>
</tr>
<tr class="row-even"><td><p><strong>Hallucination Rate</strong></p></td>
<td><p>% of hallucinated outputs</p></td>
<td><p>Aggregate measure</p></td>
<td><p>Easy to track</p></td>
<td><p>Crude metric</p></td>
<td><p>Ops dashboards</p></td>
</tr>
</tbody>
</table>
<p>👉 <strong>Industrial use</strong>: Mandatory in <strong>healthcare, finance, legal, and enterprise RAG</strong> — where fabricated facts can be costly.</p>
</section>
<hr class="docutils" />
<section id="tools-frameworks-for-hallucination-detection">
<h2>🔹 4. Tools &amp; Frameworks for Hallucination Detection<a class="headerlink" href="#tools-frameworks-for-hallucination-detection" title="Link to this heading"></a></h2>
<p>Open-source tools make it easier to integrate hallucination detection into your pipeline:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Tool</p></th>
<th class="head"><p>Category</p></th>
<th class="head"><p>What it does</p></th>
<th class="head"><p>Key hallucination metrics</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Weaknesses</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Ragas</strong></p></td>
<td><p>RAG evaluation</p></td>
<td><p>Scores faithfulness, recall, relevancy</p></td>
<td><p>Faithfulness, Context Recall, Answer Relevancy</p></td>
<td><p>Tailored for RAG; LangChain integration</p></td>
<td><p>Slower with large corpora</p></td>
</tr>
<tr class="row-odd"><td><p><strong>TruLens</strong></p></td>
<td><p>Eval + observability</p></td>
<td><p>Adds feedback functions &amp; traces</p></td>
<td><p>Groundedness, Relevance, Toxicity</p></td>
<td><p>Flexible, OpenTelemetry support</p></td>
<td><p>Needs instrumentation</p></td>
</tr>
<tr class="row-even"><td><p><strong>DeepEval</strong></p></td>
<td><p>Lightweight eval</p></td>
<td><p>Quick hallucination &amp; toxicity tests</p></td>
<td><p>Faithfulness</p></td>
<td><p>Easy to use</p></td>
<td><p>Fewer metrics</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Giskard</strong></p></td>
<td><p>Bias &amp; robustness testing</p></td>
<td><p>Custom unit tests for LLMs</p></td>
<td><p>Bias / robustness checks</p></td>
<td><p>GUI-driven</p></td>
<td><p>Less LLM-specific</p></td>
</tr>
<tr class="row-even"><td><p><strong>Langfuse</strong></p></td>
<td><p>Observability</p></td>
<td><p>Logs &amp; collects ratings</p></td>
<td><p>Human/LLM factuality</p></td>
<td><p>Great for dashboards</p></td>
<td><p>No built-in metrics</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Guardrails.ai</strong></p></td>
<td><p>Guardrails</p></td>
<td><p>Schema + PII/claim validation</p></td>
<td><p>Unsupported claims</p></td>
<td><p>Easy JSON integration</p></td>
<td><p>Rule-based only</p></td>
</tr>
<tr class="row-even"><td><p><strong>NeMo Guardrails</strong></p></td>
<td><p>Safety toolkit</p></td>
<td><p>Pre-built refusal &amp; safety flows</p></td>
<td><p>Injection/toxicity checks</p></td>
<td><p>Multi-modal; NVIDIA ecosystem</p></td>
<td><p>Heavier</p></td>
</tr>
<tr class="row-odd"><td><p><strong>OpenAI Evals</strong></p></td>
<td><p>Benchmarks</p></td>
<td><p>Eval framework</p></td>
<td><p>Factuality vs gold refs</p></td>
<td><p>Industry-tested</p></td>
<td><p>Limited customization</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="embedding-evaluation-metrics">
<h2>🔹 5. Embedding Evaluation Metrics<a class="headerlink" href="#embedding-evaluation-metrics" title="Link to this heading"></a></h2>
<p>Embeddings power <strong>RAG, semantic search, clustering, and recs</strong>. Their evaluation requires <strong>different metrics</strong>:</p>
</section>
<section id="intrinsic-metrics-vector-space-quality">
<h2>🔸 Intrinsic Metrics (Vector Space Quality)<a class="headerlink" href="#intrinsic-metrics-vector-space-quality" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Weaknesses</p></th>
<th class="head"><p>Industrial significance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Cosine Similarity</strong></p></td>
<td><p>Angular similarity between two vectors</p></td>
<td><p>Computes the cosine of the angle: `(u·v) / (</p></td>
<td><p>u</p></td>
<td><p></p></td>
<td><p>v</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Dot Product / Inner Product</strong></p></td>
<td><p>Alignment / projection of vectors</p></td>
<td><p>Raw dot product of vectors</p></td>
<td><p>Very fast; supports ANN libraries (e.g., FAISS IVF)</p></td>
<td><p>Biased by magnitude (longer vectors look closer)</p></td>
<td><p>Used in FAISS, ScaNN; recommender systems</p></td>
</tr>
<tr class="row-even"><td><p><strong>Euclidean Distance (L2)</strong></p></td>
<td><p>Geometric distance between vectors</p></td>
<td><p>`</p></td>
<td><p></p></td>
<td><p>u - v</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Manhattan Distance (L1)</strong></p></td>
<td><p>Absolute difference sum</p></td>
<td><p>`Σ</p></td>
<td><p>uᵢ - vᵢ</p></td>
<td><p>`</p></td>
<td><p>Robust to outliers; interpretable</p></td>
</tr>
<tr class="row-even"><td><p><strong>Silhouette Score</strong></p></td>
<td><p>Cohesion vs separation of clusters</p></td>
<td><p>(Avg dist within cluster – dist to nearest cluster) / max(…)</p></td>
<td><p>Easy to interpret; bounded [-1, 1]</p></td>
<td><p>Needs labels/clusters</p></td>
<td><p>Clustering validation for topic embeddings</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Davies–Bouldin Index (DBI)</strong></p></td>
<td><p>Compactness vs separation of clusters</p></td>
<td><p>Mean similarity of each cluster with its most similar other cluster</p></td>
<td><p>Fast; widely used</p></td>
<td><p>Lower interpretability</p></td>
<td><p>Comparing clustering embeddings quality</p></td>
</tr>
<tr class="row-even"><td><p><strong>Calinski–Harabasz Index (CHI)</strong></p></td>
<td><p>Variance ratio (between vs within clusters)</p></td>
<td><p>Ratio of dispersion between clusters to within</p></td>
<td><p>High values = better defined clusters</p></td>
<td><p>Biased towards many clusters</p></td>
<td><p>Evaluating large-scale embedding clusters</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="extrinsic-metrics-retrieval-performance">
<h2>🔸 Extrinsic Metrics (Retrieval Performance)<a class="headerlink" href="#extrinsic-metrics-retrieval-performance" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Weaknesses</p></th>
<th class="head"><p>Industrial significance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Recall&#64;K</strong></p></td>
<td><p>Coverage of relevant items in top-K results</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">relevant</span> <span class="pre">in</span> <span class="pre">top</span> <span class="pre">K</span> <span class="pre">/</span> <span class="pre">#</span> <span class="pre">total</span> <span class="pre">relevant</span></code></p></td>
<td><p>Critical for RAG; ensures grounding</p></td>
<td><p>Doesn’t penalize irrelevant results</p></td>
<td><p>RAG, QA, search</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Precision&#64;K</strong></p></td>
<td><p>Accuracy of top-K results</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">relevant</span> <span class="pre">in</span> <span class="pre">top</span> <span class="pre">K</span> <span class="pre">/</span> <span class="pre">K</span></code></p></td>
<td><p>Balances relevance</p></td>
<td><p>Can be misleading when K is small</p></td>
<td><p>Recommender systems, search</p></td>
</tr>
<tr class="row-even"><td><p><strong>Hit Rate&#64;K</strong></p></td>
<td><p>Whether at least one relevant item is in top-K</p></td>
<td><p>Boolean recall metric</p></td>
<td><p>Simple &amp; intuitive</p></td>
<td><p>Coarse; ignores ranking</p></td>
<td><p>Used in recsys, QA</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Mean Reciprocal Rank (MRR)</strong></p></td>
<td><p>Position of first relevant item</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">rank</span> <span class="pre">of</span> <span class="pre">first</span> <span class="pre">relevant</span></code> averaged over queries</p></td>
<td><p>Rewards early hits</p></td>
<td><p>Ignores other relevant docs</p></td>
<td><p>QA, conversational agents</p></td>
</tr>
<tr class="row-even"><td><p><strong>nDCG (Normalized Discounted Cumulative Gain)</strong></p></td>
<td><p>Graded ranking quality</p></td>
<td><p>Rewards higher-ranked relevant docs more than lower ones</p></td>
<td><p>Handles graded relevance</p></td>
<td><p>More complex to compute</p></td>
<td><p>Web search, enterprise retrieval</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="semantic-task-based-metrics">
<h2>🔸 Semantic / Task-based Metrics<a class="headerlink" href="#semantic-task-based-metrics" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Weaknesses</p></th>
<th class="head"><p>Industrial significance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>STS (Semantic Textual Similarity)</strong></p></td>
<td><p>Correlation of embeddings with human similarity judgments</p></td>
<td><p>Compute Pearson/Spearman correlation on STS datasets</p></td>
<td><p>Directly benchmarks semantic quality</p></td>
<td><p>Needs human-annotated gold datasets</p></td>
<td><p>Benchmarking new embedding models</p></td>
</tr>
<tr class="row-odd"><td><p><strong>BEIR Benchmark</strong></p></td>
<td><p>Multi-task IR benchmark</p></td>
<td><p>~19 datasets (news, scientific, QA, bio-medical, etc.)</p></td>
<td><p>Covers many domains; community standard</p></td>
<td><p>Expensive to run fully</p></td>
<td><p>Industrial search engines, RAG</p></td>
</tr>
<tr class="row-even"><td><p><strong>MTEB (Massive Text Embedding Benchmark)</strong></p></td>
<td><p>Large suite of embedding tasks (retrieval, clustering, classification)</p></td>
<td><p>50+ datasets across 8 tasks</p></td>
<td><p>Covers wide spectrum</p></td>
<td><p>Requires resources; community evolving</p></td>
<td><p>Academic + industrial model comparison</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Classification Accuracy</strong></p></td>
<td><p>Predictive quality when embeddings used as features</p></td>
<td><p>Train linear/logistic classifier over embeddings</p></td>
<td><p>Simple; fast</p></td>
<td><p>Task-specific</p></td>
<td><p>Downstream task validation</p></td>
</tr>
<tr class="row-even"><td><p><strong>Analogy Accuracy</strong></p></td>
<td><p>Relational reasoning</p></td>
<td><p>“king – man + woman = queen” style tests</p></td>
<td><p>Shows relational strength</p></td>
<td><p>More word2vec-era</p></td>
<td><p>Legacy eval; not common for modern LLM embeddings</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="operational-metrics-production-readiness">
<h2>🔸 Operational Metrics (Production Readiness)<a class="headerlink" href="#operational-metrics-production-readiness" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Weaknesses</p></th>
<th class="head"><p>Industrial significance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Index Build Time</strong></p></td>
<td><p>Efficiency of creating ANN index</p></td>
<td><p>Measure time to train/insert into FAISS, HNSW, etc.</p></td>
<td><p>Critical for large updates</p></td>
<td><p>One-time cost (less important for static data)</p></td>
<td><p>Bulk ingestion pipelines</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Query Latency (p95/p99)</strong></p></td>
<td><p>Response time distribution</p></td>
<td><p>Measure latency at 95th/99th percentile</p></td>
<td><p>Captures tail latency issues</p></td>
<td><p>Needs careful infra monitoring</p></td>
<td><p>Real-time search &amp; RAG</p></td>
</tr>
<tr class="row-even"><td><p><strong>Throughput (QPS)</strong></p></td>
<td><p>Queries per second supported at SLA</p></td>
<td><p>Load-test ANN service</p></td>
<td><p>Scales system evaluation</p></td>
<td><p>Trade-off with latency</p></td>
<td><p>Scaling production APIs</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Memory Footprint</strong></p></td>
<td><p>RAM used by index</p></td>
<td><p>Measured during index building and queries</p></td>
<td><p>Shows resource efficiency</p></td>
<td><p>May force PQ/quantization</p></td>
<td><p>Cost optimization in production</p></td>
</tr>
<tr class="row-even"><td><p><strong>Recall vs Index Size Tradeoff</strong></p></td>
<td><p>Compression vs accuracy</p></td>
<td><p>Compare PQ/HNSW/IVF index types</p></td>
<td><p>Helps engineering trade-offs</p></td>
<td><p>Requires tuning</p></td>
<td><p>Cloud-scale RAG (AWS, GCP)</p></td>
</tr>
</tbody>
</table>
<hr class="docutils" />
<p>✅ <strong>Teacher’s perspective wrap-up:</strong></p>
<ul class="simple">
<li><p><strong>Intrinsic metrics</strong> tell you if the <strong>geometry of your vector space makes sense</strong>.</p></li>
<li><p><strong>Extrinsic metrics</strong> tell you if embeddings actually <strong>retrieve the right knowledge</strong>.</p></li>
<li><p><strong>Semantic/task metrics</strong> benchmark them against <strong>real-world datasets</strong>.</p></li>
<li><p><strong>Operational metrics</strong> decide if your system is <strong>deployable at scale</strong>.</p></li>
</ul>
<p>👉 <strong>Industrial use</strong>: If you’re building a <strong>RAG pipeline</strong>, focus on Recall&#64;K + nDCG. If you’re deploying embeddings at <strong>scale</strong>, monitor latency &amp; memory on your vector DB. This is why, in industry, you never rely on <em>one</em> metric — you combine <strong>at least one from each family</strong>.</p>
</section>
<hr class="docutils" />
<section id="diversity-fluency-metrics-nlg">
<h2>🔹 6. Diversity &amp; Fluency Metrics (NLG)<a class="headerlink" href="#diversity-fluency-metrics-nlg" title="Link to this heading"></a></h2>
<p>Sometimes, outputs are factually correct but <strong>boring or repetitive</strong>. Diversity metrics capture variety and naturalness.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Industrial significance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Distinct-n</strong></p></td>
<td><p>Diversity of n-grams</p></td>
<td><p>Ratio of unique n-grams to total</p></td>
<td><p>Prevents repetitive chatbot answers</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Self-BLEU</strong></p></td>
<td><p>Diversity across multiple outputs</p></td>
<td><p>BLEU of one sample against others</p></td>
<td><p>Used in generative diversity checks</p></td>
</tr>
<tr class="row-even"><td><p><strong>Entropy</strong></p></td>
<td><p>Information richness</p></td>
<td><p>Shannon entropy over n-gram distribution</p></td>
<td><p>Good for style/creative tasks</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Grammar/Fluency checks</strong></p></td>
<td><p>Correctness of language</p></td>
<td><p>LM scoring or grammar tools</p></td>
<td><p>Important for customer-facing apps</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="calibration-confidence-metrics">
<h2>🔹 7. Calibration &amp; Confidence Metrics<a class="headerlink" href="#calibration-confidence-metrics" title="Link to this heading"></a></h2>
<p>We want models to be <strong>confident only when correct</strong>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Why it matters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Expected Calibration Error (ECE)</strong></p></td>
<td><p>Gap between confidence and accuracy</p></td>
<td><p>Bin predictions vs accuracy</p></td>
<td><p>Critical in safety-critical apps</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Brier Score</strong></p></td>
<td><p>Accuracy of probability estimates</p></td>
<td><p>MSE between predicted prob &amp; truth</p></td>
<td><p>Used in probabilistic forecasting</p></td>
</tr>
<tr class="row-even"><td><p><strong>Negative Log Likelihood (NLL)</strong></p></td>
<td><p>Log-prob of true label</p></td>
<td><p>Evaluates calibration of probability dists</p></td>
<td><p>Language modeling evaluation</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="human-evaluation-protocols">
<h2>🔹 8. Human Evaluation Protocols<a class="headerlink" href="#human-evaluation-protocols" title="Link to this heading"></a></h2>
<p>Even with automated metrics, <strong>human judgment</strong> is gold.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Likert Scales (1–5)</strong></p></td>
<td><p>Fluency, coherence, relevance</p></td>
<td><p>Simple, subjective but informative</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Pairwise Ranking</strong></p></td>
<td><p>Preference between outputs</p></td>
<td><p>Used in RLHF pipelines</p></td>
</tr>
<tr class="row-even"><td><p><strong>MQM (Multidimensional Quality Metrics)</strong></p></td>
<td><p>Industry MT standard</p></td>
<td><p>Fine-grained error categories</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Annotation Audits</strong></p></td>
<td><p>Bias, fairness, toxicity</p></td>
<td><p>Used in regulated sectors</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="bias-fairness-safety-metrics">
<h2>🔹 9. Bias, Fairness &amp; Safety Metrics<a class="headerlink" href="#bias-fairness-safety-metrics" title="Link to this heading"></a></h2>
<p>Increasingly critical in <strong>enterprise and regulated sectors</strong>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>Why it matters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Demographic Parity / Equalized Odds</strong></p></td>
<td><p>Fairness across groups</p></td>
<td><p>Regulatory audits</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Toxicity Scores</strong></p></td>
<td><p>Harmful/offensive content</p></td>
<td><p>Required for chat/social apps</p></td>
</tr>
<tr class="row-even"><td><p><strong>Stereotype Probes</strong></p></td>
<td><p>Bias in embeddings/generations</p></td>
<td><p>Prevents harmful stereotypes</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Adversarial Robustness</strong></p></td>
<td><p>Resistance to prompt injection</p></td>
<td><p>Needed for enterprise safety</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="dialogue-conversational-metrics">
<h2>🔹 10. Dialogue &amp; Conversational Metrics<a class="headerlink" href="#dialogue-conversational-metrics" title="Link to this heading"></a></h2>
<p>For <strong>multi-turn systems</strong> like chatbots and agents.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>What it measures</p></th>
<th class="head"><p>Why it matters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Coherence</strong></p></td>
<td><p>Logical flow across turns</p></td>
<td><p>Avoids broken convos</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Engagement</strong></p></td>
<td><p>User interest</p></td>
<td><p>Improves retention</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dialog Success Rate</strong></p></td>
<td><p>Task completion</p></td>
<td><p>Enterprise KPI</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Avg Turns per Task</strong></p></td>
<td><p>Efficiency</p></td>
<td><p>Optimizes costs</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="task-specific-extensions">
<h2>🔹 11. Task-Specific Extensions<a class="headerlink" href="#task-specific-extensions" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Code Generation</strong> → pass&#64;k (success rate with multiple samples).</p></li>
<li><p><strong>Summarization</strong> → Pyramid method (human scoring of content units).</p></li>
<li><p><strong>Knowledge Graph QA</strong> → Hits&#64;k, Mean Rank.</p></li>
<li><p><strong>Image Captioning</strong> → SPICE metric (semantic propositional content).</p></li>
<li><p><strong>Speech / ASR</strong> → WER (Word Error Rate), CER (Character Error Rate), MOS (Mean Opinion Score).</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="putting-it-all-together">
<h1>✅ Putting It All Together<a class="headerlink" href="#putting-it-all-together" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><strong>Start with lexical metrics</strong> (ROUGE, BLEU) if you have references.</p></li>
<li><p><strong>Move to semantic metrics</strong> (BERTScore, BLEURT) to capture meaning.</p></li>
<li><p><strong>Add hallucination/factuality metrics</strong> (Ragas, TruLens) for safety-critical apps.</p></li>
<li><p><strong>Don’t forget embeddings</strong> — measure retrieval quality (Recall&#64;K, nDCG) and ops metrics (latency, memory).</p></li>
<li><p><strong>Round it out</strong> with diversity, calibration, bias/fairness, and dialogue metrics — these ensure systems are safe, engaging, and enterprise-ready.</p></li>
</ul>
<p>👉 This way, you can understand the big picture from a teacher/engineer perspective - teaching the students/AI and seeing the <em>big picture</em>: from <strong>words → meaning → truth → diversity → embeddings → operations → safety → conversations</strong>.</p>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Neo Pragnya.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>